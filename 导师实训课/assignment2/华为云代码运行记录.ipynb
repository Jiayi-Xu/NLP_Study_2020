{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-gpu==2.0.0b1\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/2b/53/e18c5e7a2263d3581a979645a185804782e59b8e13f42b9c3c3cfb5bb503/tensorflow_gpu-2.0.0b1-cp36-cp36m-manylinux1_x86_64.whl (348.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 348.9MB 31.1MB/s ta 0:00:011  0% |▏                               | 2.1MB 79.5MB/s eta 0:00:05[K    1% |▋                               | 6.8MB 90.6MB/s eta 0:00:04              | 10.5MB 96.5MB/s eta 0:00:040:04:00:04      | 24.8MB 101.8MB/s eta 0:00:040:05.1MB/s eta 0:00:04                     | 39.9MB 92.5MB/s eta 0:00:04                     | 44.1MB 85.3MB/s eta 0:00:04MB/s eta 0:00:04MB/s eta 0:00:04██▌                          | 59.4MB 98.5MB/s eta 0:00:03         | 63.6MB 94.0MB/s eta 0:00:043MB 94.4MB/s eta 0:00:034MB 92.1MB/s eta 0:00:041MB 89.4MB/s eta 0:00:04                     | 81.0MB 91.7MB/s eta 0:00:03��████▉                        | 86.0MB 96.9MB/s eta 0:00:034MB 93.4MB/s eta 0:00:03                   | 94.5MB 95.2MB/s eta 0:00:03��██████                       | 98.9MB 90.8MB/s eta 0:00:03███████▌                      | 103.1MB 97.4MB/s eta 0:00:03   30% |██████████                      | 108.0MB 95.8MB/s eta 0:00:03       | 112.0MB 72.7MB/s eta 0:00:04                     | 116.4MB 96.6MB/s eta 0:00:03         | 118.1MB 109.0MB/s eta 0:00:03��████████▎                    | 122.5MB 93.1MB/s eta 0:00:03██▋                    | 126.2MB 91.0MB/s eta 0:00:03██████████                    | 130.4MB 97.4MB/s eta 0:00:03��▎                   | 134.3MB 103.3MB/s eta 0:00:03            | 138.2MB 93.9MB/s eta 0:00:0374.3MB/s eta 0:00:03███▌                  | 146.7MB 94.6MB/s eta 0:00:03███▉                  | 150.7MB 95.7MB/s eta 0:00:03�█████████████▎                 | 155.1MB 99.4MB/s eta 0:00:02   45% |██████████████▋                 | 159.8MB 91.3MB/s eta 0:00:03% |███████████████                 | 164.2MB 94.9MB/s eta 0:00:02   | 168.2MB 92.7MB/s eta 0:00:02    | 172.4MB 95.9MB/s eta 0:00:02��███████▏               | 176.2MB 93.3MB/s eta 0:00:02   51% |████████████████▋               | 180.7MB 87.6MB/s eta 0:00:02��████████               | 184.4MB 86.8MB/s eta 0:00:02[K    53% |█████████████████▎              | 187.9MB 98.3MB/s eta 0:00:02K    57% |██████████████████▎             | 199.3MB 99.4MB/s eta 0:00:02 |██████████████████▋             | 203.2MB 96.4MB/s eta 0:00:02�█████████             | 207.2MB 92.2MB/s eta 0:00:02�██████████▍            | 210.9MB 99.3MB/s eta 0:00:02��█▋            | 214.3MB 94.4MB/s eta 0:00:02█████            | 218.0MB 99.6MB/s eta 0:00:02��█████████████████▍           | 221.7MB 90.7MB/s eta 0:00:02█████████████████▊           | 225.4MB 97.3MB/s eta 0:00:02% |█████████████████████           | 229.0MB 98.3MB/s eta 0:00:028MB 100.2MB/s eta 0:00:02MB 58.7MB/s eta 0:00:020:02█████▎         | 242.4MB 100.0MB/s eta 0:00:02[K    70% |██████████████████████▋         | 246.1MB 96.0MB/s eta 0:00:02�██████         | 249.7MB 98.0MB/s eta 0:00:02██████▏        | 252.9MB 98.0MB/s eta 0:00:01��████████████████▋        | 257.1MB 95.5MB/s eta 0:00:01�███████████        | 261.1MB 83.0MB/s eta 0:00:02101�████████▏      | 274.7MB 95.9MB/s eta 0:00:01��████████▌      | 278.4MB 87.3MB/s eta 0:00:010% |█████████████████████████▉      | 281.1MB 89.9MB/s eta 0:00:01�███████▏     | 284.9MB 86.9MB/s eta 0:00:01:01��████████▉     | 292.0MB 96.9MB/s eta 0:00:01�████████     | 294.9MB 70.2MB/s eta 0:00:01████████▊    | 301.8MB 98.3MB/s eta 0:00:01��████████████████████    | 304.9MB 89.0MB/s eta 0:00:01% |████████████████████████████▎   | 307.8MB 89.5MB/s eta 0:00:01████████████████████████████▌   | 311.2MB 97.5MB/s eta 0:00:01    90% |████████████████████████████▉   | 314.5MB 95.2MB/s eta 0:00:01�█████████████▏  | 317.8MB 96.6MB/s eta 0:00:01.6MB 67.7MB/s eta 0:00:01███████████████  | 326.7MB 97.8MB/s eta 0:00:01███████▏ | 329.4MB 83.3MB/s eta 0:00:01███████▍ | 331.8MB 96.2MB/s eta 0:00:01███████▊ | 335.4MB 97.0MB/s eta 0:00:01████████ | 338.8MB 100.0MB/s eta 0:00:01▎| 341.5MB 88.4MB/s eta 0:00:01███████████████████| 348.3MB 108.4MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0b1)\n",
      "Collecting tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501 (from tensorflow-gpu==2.0.0b1)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/32/dd/99c47dd007dcf10d63fd895611b063732646f23059c618a373e85019eb0e/tf_estimator_nightly-1.14.0.dev2019060501-py2.py3-none-any.whl (496kB)\n",
      "\u001b[K    100% |████████████████████████████████| 501kB 103.8MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting wrapt>=1.11.1 (from tensorflow-gpu==2.0.0b1)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/82/f7/e43cefbe88c5fd371f4cf0cf5eb3feccd07515af9fd6cf7dbf1d1793a797/wrapt-1.12.1.tar.gz\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0b1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0b1)\n",
      "Collecting tb-nightly<1.14.0a20190604,>=1.14.0a20190603 (from tensorflow-gpu==2.0.0b1)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/a4/96/571b875cd81dda9d5dfa1422a4f9d749e67c0a8d4f4f0b33a4e5f5f35e27/tb_nightly-1.14.0a20190603-py3-none-any.whl (3.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.1MB 99.2MB/s eta 0:00:01��▍| 3.1MB 105.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0b1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0b1)\n",
      "Collecting absl-py>=0.7.0 (from tensorflow-gpu==2.0.0b1)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/1a/53/9243c600e047bd4c3df9e69cfabc1e8004a82cac2e0c484580a78a94ba2a/absl-py-0.9.0.tar.gz (104kB)\n",
      "\u001b[K    100% |████████████████████████████████| 112kB 99.0MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0b1)\n",
      "Requirement already satisfied: astor>=0.6.0 in /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0b1)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0b1)\n",
      "Collecting google-pasta>=0.1.6 (from tensorflow-gpu==2.0.0b1)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl (57kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 77.9MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.14.5 in /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0b1)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0b1)\n",
      "Requirement already satisfied: h5py in /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0b1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0b1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0b1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0b1)\n",
      "Building wheels for collected packages: wrapt, absl-py\n",
      "  Running setup.py bdist_wheel for wrapt ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ma-user/.cache/pip/wheels/a7/47/3e/319c41219c933f06ddb6c4227c5a0d7523b8a3ea114664dd06\n",
      "  Running setup.py bdist_wheel for absl-py ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ma-user/.cache/pip/wheels/32/8f/6b/3b21b4a9c89d56416e593a3257364e3afc7db8bd5d50bd1548\n",
      "Successfully built wrapt absl-py\n",
      "Installing collected packages: tf-estimator-nightly, wrapt, absl-py, tb-nightly, google-pasta, tensorflow-gpu\n",
      "  Found existing installation: wrapt 1.10.11\n",
      "\u001b[31m    DEPRECATION: Uninstalling a distutils installed project (wrapt) has been deprecated and will be removed in a future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the project.\u001b[0m\n",
      "    Uninstalling wrapt-1.10.11:\n",
      "      Successfully uninstalled wrapt-1.10.11\n",
      "  Found existing installation: absl-py 0.2.2\n",
      "    Uninstalling absl-py-0.2.2:\n",
      "      Successfully uninstalled absl-py-0.2.2\n",
      "  Found existing installation: tensorflow-gpu 1.13.1\n",
      "    Uninstalling tensorflow-gpu-1.13.1:\n",
      "      Successfully uninstalled tensorflow-gpu-1.13.1\n",
      "Successfully installed absl-py-0.9.0 google-pasta-0.2.0 tb-nightly-1.14.0a20190603 tensorflow-gpu-2.0.0b1 tf-estimator-nightly-1.14.0.dev2019060501 wrapt-1.12.1\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 20.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 安装tensorflow2.0\n",
    "!pip install tensorflow-gpu==2.0.0b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "physical_devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以下代码执行时候参数配置信息如下：\n",
    "\n",
    "embed_size,enc_units,dec_units,attn_units 设置为256\n",
    "\n",
    "name | 第一次run配置信息 |   第二次run配置信息  \n",
    "-|-|-\n",
    "batch_size | 200 | 200 |\n",
    "vocab_size | 10000 | 30000 |\n",
    "steps_per_epoch | 1 | 1300 |\n",
    "epochs | 10 | 20 |\n",
    "checkpoint路径|checkpoint|checkpoint_vocab30000|\n",
    "Loss |3.3990|2.98|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_size of vocab was specified as 10000; we now have 10000 words. Stopping reading.\n",
      "Finished constructing vocabulary of 10000 total words. Last word added: 私\n",
      "true vocab is  10000\n",
      "Creating the batcher ...\n",
      "WARNING:tensorflow:From /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py:505: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n",
      "Building the model ...\n",
      "WARNING:tensorflow:From /home/ma-user/work/test/seq2seq_tf2/encoders/rnn_encoder.py:38: The name tf.keras.layers.CuDNNGRU is deprecated. Please use tf.compat.v1.keras.layers.CuDNNGRU instead.\n",
      "\n",
      "Creating the checkpoint manager\n",
      "Initializing from scratch.\n",
      "Starting the training ...\n",
      "WARNING:tensorflow:From /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1220: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1 Batch 100 Loss 6.4773\n",
      "Epoch 1 Batch 200 Loss 6.2184\n",
      "Epoch 1 Batch 300 Loss 6.0554\n",
      "Epoch 1 Batch 400 Loss 5.9082\n",
      "Saving checkpoint for epoch 1 at /home/ma-user/work/test/ckpt/seq2seq/checkpoint/ckpt-1 ,best loss 5.888243675231934\n",
      "Epoch 1 Loss 5.8882\n",
      "Time taken for 1 epoch 397.12049412727356 sec\n",
      "\n",
      "Epoch 2 Batch 100 Loss 5.0965\n",
      "Epoch 2 Batch 200 Loss 5.0356\n",
      "Epoch 2 Batch 300 Loss 4.9934\n",
      "Epoch 2 Batch 400 Loss 4.9363\n",
      "Epoch 3 Batch 100 Loss 4.5208\n",
      "Epoch 3 Batch 200 Loss 4.4839\n",
      "Epoch 3 Batch 300 Loss 4.4712\n",
      "Epoch 3 Batch 400 Loss 4.4450\n",
      "Saving checkpoint for epoch 3 at /home/ma-user/work/test/ckpt/seq2seq/checkpoint/ckpt-2 ,best loss 4.439842700958252\n",
      "Epoch 3 Loss 4.4398\n",
      "Time taken for 1 epoch 396.7885580062866 sec\n",
      "\n",
      "Epoch 4 Batch 100 Loss 4.1772\n",
      "Epoch 4 Batch 200 Loss 4.1534\n",
      "Epoch 4 Batch 300 Loss 4.1543\n",
      "Epoch 4 Batch 400 Loss 4.1440\n",
      "Epoch 5 Batch 100 Loss 3.9456\n",
      "Epoch 5 Batch 200 Loss 3.9292\n",
      "Epoch 5 Batch 300 Loss 3.9378\n",
      "Epoch 5 Batch 400 Loss 3.9375\n",
      "Saving checkpoint for epoch 5 at /home/ma-user/work/test/ckpt/seq2seq/checkpoint/ckpt-3 ,best loss 3.935612678527832\n",
      "Epoch 5 Loss 3.9356\n",
      "Time taken for 1 epoch 390.69515347480774 sec\n",
      "\n",
      "Epoch 6 Batch 100 Loss 3.7800\n",
      "Epoch 6 Batch 200 Loss 3.7668\n",
      "Epoch 6 Batch 300 Loss 3.7796\n",
      "Epoch 6 Batch 400 Loss 3.7849\n",
      "Epoch 7 Batch 100 Loss 3.6505\n",
      "Epoch 7 Batch 200 Loss 3.6398\n",
      "Epoch 7 Batch 300 Loss 3.6547\n",
      "Epoch 7 Batch 400 Loss 3.6630\n",
      "Saving checkpoint for epoch 7 at /home/ma-user/work/test/ckpt/seq2seq/checkpoint/ckpt-4 ,best loss 3.6622798442840576\n",
      "Epoch 7 Loss 3.6623\n",
      "Time taken for 1 epoch 388.95069551467896 sec\n",
      "\n",
      "Epoch 8 Batch 100 Loss 3.5400\n",
      "Epoch 8 Batch 200 Loss 3.5341\n",
      "Epoch 8 Batch 300 Loss 3.5497\n",
      "Epoch 8 Batch 400 Loss 3.5598\n",
      "Epoch 9 Batch 100 Loss 3.4445\n",
      "Epoch 9 Batch 200 Loss 3.4442\n",
      "Epoch 9 Batch 300 Loss 3.4602\n",
      "Epoch 9 Batch 400 Loss 3.4720\n",
      "Saving checkpoint for epoch 9 at /home/ma-user/work/test/ckpt/seq2seq/checkpoint/ckpt-5 ,best loss 3.4714741706848145\n",
      "Epoch 9 Loss 3.4715\n",
      "Time taken for 1 epoch 384.0371572971344 sec\n",
      "\n",
      "Epoch 10 Batch 100 Loss 3.3641\n",
      "Epoch 10 Batch 200 Loss 3.3686\n",
      "Epoch 10 Batch 300 Loss 3.3853\n",
      "Epoch 10 Batch 400 Loss 3.3990\n"
     ]
    }
   ],
   "source": [
    "%run test/seq2seq_tf2/bin/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_size of vocab was specified as 30000; we now have 30000 words. Stopping reading.\n",
      "Finished constructing vocabulary of 30000 total words. Last word added: 查些\n",
      "true vocab is  30000\n",
      "Creating the batcher ...\n",
      "WARNING:tensorflow:From /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py:505: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n",
      "Building the model ...\n",
      "WARNING:tensorflow:From /home/ma-user/work/test/seq2seq_tf2/encoders/rnn_encoder.py:38: The name tf.keras.layers.CuDNNGRU is deprecated. Please use tf.compat.v1.keras.layers.CuDNNGRU instead.\n",
      "\n",
      "Creating the checkpoint manager\n",
      "Initializing from scratch.\n",
      "Starting the training ...\n",
      "WARNING:tensorflow:From /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1220: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1 Batch 100 Loss 6.8364\n",
      "Epoch 1 Batch 200 Loss 6.5912\n",
      "Epoch 1 Batch 300 Loss 6.4468\n",
      "Epoch 1 Batch 400 Loss 6.3083\n",
      "Saving checkpoint for epoch 1 at /home/ma-user/work/test/ckpt/seq2seq/checkpoint_vocab30000/ckpt-1 ,best loss 6.289400577545166\n",
      "Epoch 1 Loss 6.2894\n",
      "Time taken for 1 epoch 435.69306349754333 sec\n",
      "\n",
      "Epoch 2 Batch 100 Loss 5.4626\n",
      "Epoch 2 Batch 200 Loss 5.3963\n",
      "Epoch 2 Batch 300 Loss 5.3438\n",
      "Epoch 2 Batch 400 Loss 5.2781\n",
      "Epoch 3 Batch 100 Loss 4.8016\n",
      "Epoch 3 Batch 200 Loss 4.7606\n",
      "Epoch 3 Batch 300 Loss 4.7380\n",
      "Epoch 3 Batch 400 Loss 4.7040\n",
      "Saving checkpoint for epoch 3 at /home/ma-user/work/test/ckpt/seq2seq/checkpoint_vocab30000/ckpt-2 ,best loss 4.697807312011719\n",
      "Epoch 3 Loss 4.6978\n",
      "Time taken for 1 epoch 429.02867102622986 sec\n",
      "\n",
      "Epoch 4 Batch 100 Loss 4.3963\n",
      "Epoch 4 Batch 200 Loss 4.3675\n",
      "Epoch 4 Batch 300 Loss 4.3627\n",
      "Epoch 4 Batch 400 Loss 4.3469\n",
      "Epoch 5 Batch 100 Loss 4.1244\n",
      "Epoch 5 Batch 200 Loss 4.1029\n",
      "Epoch 5 Batch 300 Loss 4.1083\n",
      "Epoch 5 Batch 400 Loss 4.1030\n",
      "Saving checkpoint for epoch 5 at /home/ma-user/work/test/ckpt/seq2seq/checkpoint_vocab30000/ckpt-3 ,best loss 4.100196361541748\n",
      "Epoch 5 Loss 4.1002\n",
      "Time taken for 1 epoch 429.2133905887604 sec\n",
      "\n",
      "Epoch 6 Batch 100 Loss 3.9264\n",
      "Epoch 6 Batch 200 Loss 3.9102\n",
      "Epoch 6 Batch 300 Loss 3.9219\n",
      "Epoch 6 Batch 400 Loss 3.9232\n",
      "Epoch 7 Batch 100 Loss 3.7733\n",
      "Epoch 7 Batch 200 Loss 3.7610\n",
      "Epoch 7 Batch 300 Loss 3.7759\n",
      "Epoch 7 Batch 400 Loss 3.7811\n",
      "Saving checkpoint for epoch 7 at /home/ma-user/work/test/ckpt/seq2seq/checkpoint_vocab30000/ckpt-4 ,best loss 3.779501438140869\n",
      "Epoch 7 Loss 3.7795\n",
      "Time taken for 1 epoch 422.93005657196045 sec\n",
      "\n",
      "Epoch 8 Batch 100 Loss 3.6461\n",
      "Epoch 8 Batch 200 Loss 3.6381\n",
      "Epoch 8 Batch 300 Loss 3.6547\n",
      "Epoch 8 Batch 400 Loss 3.6625\n",
      "Epoch 9 Batch 100 Loss 3.5370\n",
      "Epoch 9 Batch 200 Loss 3.5350\n",
      "Epoch 9 Batch 300 Loss 3.5519\n",
      "Epoch 9 Batch 400 Loss 3.5619\n",
      "Saving checkpoint for epoch 9 at /home/ma-user/work/test/ckpt/seq2seq/checkpoint_vocab30000/ckpt-5 ,best loss 3.5606625080108643\n",
      "Epoch 9 Loss 3.5607\n",
      "Time taken for 1 epoch 426.7815704345703 sec\n",
      "\n",
      "Epoch 10 Batch 100 Loss 3.4452\n",
      "Epoch 10 Batch 200 Loss 3.4493\n",
      "Epoch 10 Batch 300 Loss 3.4663\n",
      "Epoch 10 Batch 400 Loss 3.4778\n",
      "Epoch 11 Batch 100 Loss 3.3630\n",
      "Epoch 11 Batch 200 Loss 3.3722\n",
      "Epoch 11 Batch 300 Loss 3.3909\n",
      "Epoch 11 Batch 400 Loss 3.4040\n",
      "Saving checkpoint for epoch 11 at /home/ma-user/work/test/ckpt/seq2seq/checkpoint_vocab30000/ckpt-6 ,best loss 3.40301513671875\n",
      "Epoch 11 Loss 3.4030\n",
      "Time taken for 1 epoch 424.7373380661011 sec\n",
      "\n",
      "Epoch 12 Batch 100 Loss 3.2968\n",
      "Epoch 12 Batch 200 Loss 3.3078\n",
      "Epoch 12 Batch 300 Loss 3.3270\n",
      "Epoch 12 Batch 400 Loss 3.3409\n",
      "Epoch 13 Batch 100 Loss 3.2355\n",
      "Epoch 13 Batch 200 Loss 3.2443\n",
      "Epoch 13 Batch 300 Loss 3.2659\n",
      "Epoch 13 Batch 400 Loss 3.2813\n",
      "Saving checkpoint for epoch 13 at /home/ma-user/work/test/ckpt/seq2seq/checkpoint_vocab30000/ckpt-7 ,best loss 3.280855894088745\n",
      "Epoch 13 Loss 3.2809\n",
      "Time taken for 1 epoch 424.2514250278473 sec\n",
      "\n",
      "Epoch 14 Batch 100 Loss 3.1877\n",
      "Epoch 14 Batch 200 Loss 3.1924\n",
      "Epoch 14 Batch 300 Loss 3.2142\n",
      "Epoch 14 Batch 400 Loss 3.2296\n",
      "Epoch 15 Batch 100 Loss 3.1421\n",
      "Epoch 15 Batch 200 Loss 3.1441\n",
      "Epoch 15 Batch 300 Loss 3.1662\n",
      "Epoch 15 Batch 400 Loss 3.1826\n",
      "Saving checkpoint for epoch 15 at /home/ma-user/work/test/ckpt/seq2seq/checkpoint_vocab30000/ckpt-8 ,best loss 3.1820578575134277\n",
      "Epoch 15 Loss 3.1821\n",
      "Time taken for 1 epoch 423.4413652420044 sec\n",
      "\n",
      "Epoch 16 Batch 100 Loss 3.1000\n",
      "Epoch 16 Batch 200 Loss 3.1059\n",
      "Epoch 16 Batch 300 Loss 3.1264\n",
      "Epoch 16 Batch 400 Loss 3.1421\n",
      "Epoch 17 Batch 100 Loss 3.0566\n",
      "Epoch 17 Batch 200 Loss 3.0673\n",
      "Epoch 17 Batch 300 Loss 3.0865\n",
      "Epoch 17 Batch 400 Loss 3.1012\n",
      "Saving checkpoint for epoch 17 at /home/ma-user/work/test/ckpt/seq2seq/checkpoint_vocab30000/ckpt-9 ,best loss 3.100086212158203\n",
      "Epoch 17 Loss 3.1001\n",
      "Time taken for 1 epoch 424.17457842826843 sec\n",
      "\n",
      "Epoch 18 Batch 100 Loss 3.0131\n",
      "Epoch 18 Batch 200 Loss 3.0246\n",
      "Epoch 18 Batch 300 Loss 3.0433\n",
      "Epoch 18 Batch 400 Loss 3.0586\n",
      "Epoch 19 Batch 100 Loss 2.9720\n",
      "Epoch 19 Batch 200 Loss 2.9824\n",
      "Epoch 19 Batch 300 Loss 3.0039\n",
      "Epoch 19 Batch 400 Loss 3.0202\n",
      "Saving checkpoint for epoch 19 at /home/ma-user/work/test/ckpt/seq2seq/checkpoint_vocab30000/ckpt-10 ,best loss 3.019021987915039\n",
      "Epoch 19 Loss 3.0190\n",
      "Time taken for 1 epoch 423.668169260025 sec\n",
      "\n",
      "Epoch 20 Batch 100 Loss 2.9404\n",
      "Epoch 20 Batch 200 Loss 2.9489\n",
      "Epoch 20 Batch 300 Loss 2.9698\n",
      "Epoch 20 Batch 400 Loss 2.9867\n"
     ]
    }
   ],
   "source": [
    "%run test/seq2seq_tf2/bin/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-1.13.1",
   "language": "python",
   "name": "tensorflow-1.13.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

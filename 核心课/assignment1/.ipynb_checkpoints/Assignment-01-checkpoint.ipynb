{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson-01 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本次作业的内容 - 后厂理工学院"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 复现课堂代码\n",
    "\n",
    "在本部分，你需要参照我们给大家的GitHub地址里边的课堂代码，结合课堂内容，复现内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 完成以下问答和编程练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础理论部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Can you come up out 3 sceneraies which use AI methods? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: \n",
    "+ 1.语音识别：苹果里面的siri，阿里天猫精灵\n",
    "+ 2.人脸识别：苹果手机刷脸解锁\n",
    "+ 3.根据历史的信息去预测发电厂每天应该发多少量（时序问题）：不然发电少了供不应求会减少收入，发电多了需要储备消耗一些成本去储备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. How do we use Github; Why do we use Jupyter and Pycharm;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: \n",
    "### github使用\n",
    "+ git clone url 下载项目到本地\n",
    "+ git branch dev 创建dev分支\n",
    "+ git checkout dev 切换到该分支，开始本地进行编辑\n",
    "##### 提交本地变更到远程项目重要步骤\n",
    "+ git add .\n",
    "+ git commit -m \"description\"\n",
    "+ git push\n",
    "+ git checkout master 切换到master\n",
    "+ git pull origin master 在合并之前先把远程代码下载下来\n",
    "+ git merge dev 把dev分支的代码合并到master上\n",
    "+ git push 推送合并后的带啊吗\n",
    "\n",
    "+ git pull 下载远程代码到本地\n",
    "\n",
    "\n",
    "### 为什么使用jupyter和pycharm\n",
    "1.jupyter支持执行代码查看运行结果和编辑markdown，可以方便的用来整理和分享记录学习步骤。\n",
    "\n",
    "2.pycharm作为集成开发环境的一种，可以提高我们的开发效率，比如调试、语法高亮、项目管理、代码跳转、智能提示、关键字的自动补全、单元测试、版本控制等，帮助更快的完成项目。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What's the Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 由条件概率表示的模型为概率模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Can you came up with some sceneraies at which we could use Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "1. 计算n枚硬币是正面朝上的概率\n",
    "2. 抽样（有放回或者无放回）产品，计算产品不合格率\n",
    "3. 生日问题：n个人生日不相同的概率\n",
    "\n",
    "等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "因为现实问题很多解决方案需要基于以前的数据预测之后的数据，需要用概率高的结果。\n",
    "\n",
    "存在难点：中文词不像英文靠空格分词，而是连起来的，在用模式匹配分词时难度会高一些。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What's the Language Model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "为了验证一句话是否合理，通过分析已有被大多数认可的文本数据对其中的语句进行概率分布的建模，比如一个句子包含w1，w2，...，$w_{n}$个词，语言模型就是计算该序列的概率P(w1,w2,..,$w_{n}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Can you came up with some sceneraies at which we could use Language Model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "+ 判断一个语言序列是否是正常语句 比如P(I am a boy)概率会大于P(a boy I am)\n",
    "+ 中文分词：对句子A：“已结婚的和尚未结婚的青年”进行分词，P(已/ 结婚/ 的/ 和/ 尚未/ 结婚/ 的/ 青年 | A) 概率大于 P(已/ 结婚/ 的/ 和尚/ 未/ 结婚/ 的/ 青年 | A)\n",
    "+ 机器翻译：P(high winds tonight | 今晚有大风) 概率大于 P(large winds tonight | 今晚有大风)\n",
    "+ 语音识别：识别语音到句子 P(I saw a van) 概率会大于 P(eyes awe of an)\n",
    "\n",
    "等 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. What's the 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 根据链式法则，第n个出现的词基于前面出现的词，\n",
    "\n",
    "$$ P(w1,w2,...,w_{n}) = P(w1)P(w2|w1)P(w3|w1,w2)...P(w_{n}|w1,w2,...,w_{n-1})$$\n",
    "\n",
    "当句子很长时候，会出现稀疏性问题，计算量也会很大，为了解决这样的问题，引入马尔可夫假设，当前词出现的概率依赖于前n-1个词，则\n",
    "\n",
    "$$ P(w_{i}|w1,w2,...,w_{i-1} = P(w_{i}|w_{i-n+1},...,w_{i-1})$$\n",
    "\n",
    "基于上式，1-gram则为n=1\n",
    "\n",
    "$$ P(w1,w2,...,w_{n})= \\prod_{i=1}^n P(w_{i}) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. What's the disadvantages and advantages of 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "比起只使用链式法则，1-gram模型解决了稀疏性问题，但是它只考虑了当前词本身出现的概率，而不考虑当前词的上下文环境，对下一个词出现的约束性信息会比较小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. What't the 2-gram models;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "基于第七题的马尔可夫假设，假设下一个词的出现依赖它前面的一个词,2-gram则为n=2\n",
    "\n",
    "$$ P(w1,w2,...,w_{n})= \\prod_{i=1}^n P(w_{i}|w_{i-1}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编程实践部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 设计你自己的句子生成器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何生成句子是一个很经典的问题，从1940s开始，图灵提出机器智能的时候，就使用的是人类能不能流畅和计算机进行对话。和计算机对话的一个前提是，计算机能够生成语言。\n",
    "\n",
    "计算机如何能生成语言是一个经典但是又很复杂的问题。 我们课程上为大家介绍的是一种基于规则（Rule Based）的生成方法。该方法虽然提出的时间早，但是现在依然在很多地方能够大显身手。值得说明的是，现在很多很实用的算法，都是很久之前提出的，例如，二分查找提出与1940s, Dijstra算法提出于1960s 等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这一部分，需要各位同学首先定义自己的语言。 大家可以先想一个应用场景，然后在这个场景下，定义语法。例如：\n",
    "\n",
    "在西部世界里，一个”人类“的语言可以定义为：\n",
    "``` \n",
    "human = \"\"\"\n",
    "human = 自己 寻找 活动\n",
    "自己 = 我 | 俺 | 我们 \n",
    "寻找 = 看看 | 找找 | 想找点\n",
    "活动 = 乐子 | 玩的\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "一个“接待员”的语言可以定义为\n",
    "```\n",
    "host = \"\"\"\n",
    "host = 寒暄 报数 询问 业务相关 结尾 \n",
    "报数 = 我是 数字 号 ,\n",
    "数字 = 单个数字 | 数字 单个数字 \n",
    "单个数字 = 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \n",
    "寒暄 = 称谓 打招呼 | 打招呼\n",
    "称谓 = 人称 ,\n",
    "人称 = 先生 | 女士 | 小朋友\n",
    "打招呼 = 你好 | 您好 \n",
    "询问 = 请问你要 | 您需要\n",
    "业务相关 = 玩玩 具体业务\n",
    "玩玩 = 耍一耍 | 玩一玩\n",
    "具体业务 = 喝酒 | 打牌 | 打猎 | 赌博\n",
    "结尾 = 吗？\"\"\"\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请定义你自己的语法: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "EatOut = '''\n",
    "begin = 时间 人物 询问 询问2 询问3 结尾 \n",
    "时间 = 今天 时刻| 明天 时刻| 后天 时刻| 时刻\n",
    "时刻 = null | 中午 | 晚上 | 傍晚\n",
    "人物 = 你 | 我们 | 他们\n",
    "询问 = 去吃 吃的活动 | 去 活动 | 去看 看的活动\n",
    "询问2 = null | 然后再 询问\n",
    "询问3 = null | 或者 询问\n",
    "活动 = 骑车 | 逛街 | 游乐场玩 | 爬山 | 打球\n",
    "吃的活动 = 火锅 | 小龙虾 | 杭帮菜  | 烤肉 | 烧烤\n",
    "看的活动 = 展览 | 电影 | 动漫展\n",
    "结尾 = 吗？| 吧？ | 嚒？\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hair = '''\n",
    "begin = 问候 报名字 询问 等级 给您 业务相关 结尾 \n",
    "报名字 = 我是 名字 ,\n",
    "名字 = Tony | Joe | Amy | Zoe\n",
    "问候 = 称谓 打招呼 | 打招呼\n",
    "称谓 = 人称 ,\n",
    "人称 = 小姐姐 | 美女 | 小哥哥 | 帅哥\n",
    "打招呼 = 欢迎光临 | 您好 | 你好 \n",
    "询问 = 请问你要 | 您需要\n",
    "等级 = 设计总监 | 设计师\n",
    "业务相关 = 理发 | 烫头发 | 修眉 | 剪发 | 染发\n",
    "结尾 = 吗？\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 然后，使用自己之前定义的generate函数，使用此函数生成句子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "choice = random.choice\n",
    "\n",
    "# 解析语法\n",
    "def create_grammar(grammar_str, split='=', line_split='\\n'):\n",
    "    grammar = {}\n",
    "    for line in grammar_str.split(line_split):\n",
    "        if not line.strip(): continue\n",
    "        exp, stmt = line.split(split)\n",
    "        grammar[exp.strip()] = [s.split() for s in stmt.split('|')]\n",
    "    return grammar\n",
    "def generate(gram, target='begin'):\n",
    "    if target not in gram: return target\n",
    "    \n",
    "    expaned = [generate(gram, t) for t in choice(gram[target])]\n",
    "    return ''.join([e if e != '/n' else '\\n' for e in expaned if e != 'null'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'明天傍晚我们去吃杭帮菜然后再去吃火锅吧？'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eat_grammar = create_grammar(EatOut)\n",
    "generate(gram=eat_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'您好我是Amy,您需要设计总监给您染发吗？'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hair_grammar = create_grammar(Hair)\n",
    "generate(gram=hair_grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 然后，定义一个函数，generate_n，将generate扩展，使其能够生成n个句子:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['后天我们去看动漫展吗？', '明天中午我们去看电影嚒？', '中午我们去吃火锅然后再去骑车或者去吃烤肉吗？', '晚上我们去看动漫展吗？', '晚上我们去吃烤肉然后再去吃烤肉或者去打球吗？', '后天晚上他们去打球然后再去吃烧烤或者去吃烧烤吗？', '你去爬山或者去打球嚒？', '今天中午你去游乐场玩然后再去看电影吗？', '中午你去逛街或者去爬山吗？', '明天我们去看动漫展吧？']\n"
     ]
    }
   ],
   "source": [
    "def generate_n(n,gram_name):\n",
    "    sentences = []\n",
    "    for i in range(n):\n",
    "        sentences.append(generate(gram=gram_name))\n",
    "    return sentences\n",
    "# 测试\n",
    "s = generate_n(10,eat_grammar)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 使用新数据源完成语言模型的训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照我们上文中定义的`prob_2`函数，我们更换一个文本数据源，获得新的Language Model:\n",
    "\n",
    "1. 下载文本数据集（你可以在以下数据集中任选一个，也可以两个都使用）\n",
    "    + 可选数据集1，保险行业问询对话集： https://github.com/Computing-Intelligence/insuranceqa-corpus-zh/raw/release/corpus/pool/train.txt.gz\n",
    "    + 可选数据集2：豆瓣评论数据集：https://github.com/Computing-Intelligence/datasource/raw/master/movie_comments.csv\n",
    "2. 修改代码，获得新的**2-gram**语言模型\n",
    "    + 进行文本清洗，获得所有的纯文本\n",
    "    + 将这些文本进行切词\n",
    "    + 送入之前定义的语言模型中，判断文本的合理程度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 获得最优质的的语言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们能够生成随机的语言并且能判断之后，我们就可以生成更加合理的语言了。请定义 generate_best 函数，该函数输入一个语法 + 语言模型，能够生成**n**个句子，并能选择一个最合理的句子\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 豆瓣评论数据集 2-gram作业\n",
    "\n",
    "步骤记录：\n",
    "+ 1.导入文件movie_comments.csv\n",
    "+ 2.进行文本清洗，获得所有的纯文本\n",
    "+ 3.将这些文本进行切词\n",
    "+ 4.送入定义的2-gram语言模型中，判断文本的合理程度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maytone/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "filename = '../../../datasource/movie_comments.csv'\n",
    "content = pd.read_csv(filename, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京意淫到了脑残的地步，看了恶心想吐</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>中二得很</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                        link name  \\\n",
       "0  1  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "1  2  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "2  3  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "3  4  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "4  5  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "\n",
       "                                             comment star  \n",
       "0                                 吴京意淫到了脑残的地步，看了恶心想吐    1  \n",
       "1  首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...    2  \n",
       "2  吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...    2  \n",
       "3                      凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。    4  \n",
       "4                                               中二得很    1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看导入的数据集信息\n",
    "content.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261497\n",
      "['吴京意淫到了脑残的地步，看了恶心想吐', '首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮番上场，视物理逻辑于不顾，不得不说有钱真好，随意胡闹', '吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋律，为了煽情而煽情，让人觉得他是个大做作、大谎言家。（7.29更新）片子整体不如湄公河行动，1.整体不够流畅，编剧有毒，台词尴尬；2.刻意做作的主旋律煽情显得如此不合时宜而又多余。', '凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。', '中二得很']\n"
     ]
    }
   ],
   "source": [
    "comments = content['comment'].tolist()\n",
    "print(len(comments))\n",
    "print(comments[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 进行数据清洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import jieba\n",
    "from collections import Counter\n",
    "\n",
    "# 定义函数token 去掉字符\n",
    "def token(string):\n",
    "    return re.findall('\\w+', string) # 使用正则提取出中文数据\n",
    "# 定义函数cut 使用jieba进行分词\n",
    "def cut(string): \n",
    "    return list(jieba.cut(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('comments.txt','w')\n",
    "\n",
    "for sentence in comments:\n",
    "    comment_clean = ''.join(token(str(sentence))) + '\\n'\n",
    "#     print(comment_clean)\n",
    "    f.write(comment_clean)\n",
    "# 写完数据后关闭文件\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_clean = [''.join(token(str(a))) for a in comments]\n",
    "with open('comments.txt', 'w') as f:\n",
    "    for a in comments_clean:\n",
    "        f.write(a + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/wh/n0tnmx7s7rz03bb00g19hn1m0000gn/T/jieba.cache\n",
      "Loading model cost 1.184 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['之外', '唯一', '的', '感觉', '就是', '他', '男朋友', '真让人', '恶心', '\\n']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKEN_1_GRAM = []\n",
    "for i, line in enumerate((open('comments.txt'))):   \n",
    "    TOKEN_1_GRAM += cut(line)\n",
    "# 展示单个词例子\n",
    "TOKEN_1_GRAM[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('的', 328262),\n",
       " ('\\n', 261497),\n",
       " ('了', 102420),\n",
       " ('是', 73106),\n",
       " ('我', 50338),\n",
       " ('都', 36255),\n",
       " ('很', 34712),\n",
       " ('看', 34022),\n",
       " ('电影', 33675),\n",
       " ('也', 32065)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_count_1 = Counter(TOKEN_1_GRAM)\n",
    "words_count_1.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('了\\n', 22033),\n",
       " ('的\\n', 10158),\n",
       " ('的电影', 8604),\n",
       " ('啊\\n', 8389),\n",
       " ('看的', 7106),\n",
       " ('\\n我', 7029),\n",
       " ('都是', 6335),\n",
       " ('让人', 5284),\n",
       " ('的故事', 4673),\n",
       " ('看了', 4585)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TOKEN = [str(t) for t in TOKEN]\n",
    "TOKEN_2_GRAM = [''.join(TOKEN_1_GRAM[i:i+2]) for i in range(len(TOKEN_1_GRAM[:-2]))]\n",
    "words_count_2 = Counter(TOKEN_2_GRAM)\n",
    "words_count_2.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算1-gram/2-gram\n",
    "def one_gram(word):\n",
    "    if word in words_count_1:\n",
    "        return words_count_1[word] / len(TOKEN_1_GRAM)\n",
    "    else:\n",
    "        # 使用平滑法对不存在的词进行处理\n",
    "        return 1 / len(TOKEN_1_GRAM)\n",
    "\n",
    "def two_gram(word1, word2):\n",
    "    if word1 + word2 in words_count_2: \n",
    "        # 后一个单词出现的概率依赖于前一个单词\n",
    "        return words_count_2[word1+word2] / words_count_1[word1]\n",
    "    else:\n",
    "        # 使用平滑法对不存在的词进行处理\n",
    "        return 1 / len(TOKEN_2_GRAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probablity_1gram(sentence):\n",
    "#     对句子进行分词处理\n",
    "    words = cut(sentence)\n",
    "#     初始化该句子的概率为1\n",
    "    sentence_pro = 1\n",
    "#     遍历句子的词获得句子的概率\n",
    "    # 1-gram模型\n",
    "    for word in words:\n",
    "        probability = one_gram(word)\n",
    "        sentence_pro *= probability\n",
    "    return sentence_pro\n",
    "\n",
    "\n",
    "def get_probablity_2gram(sentence):\n",
    "#     对句子进行分词处理\n",
    "    words = cut(sentence)\n",
    "#     初始化该句子的概率为1\n",
    "    sentence_pro = 1\n",
    "#     遍历句子的词获得句子的概率\n",
    "    # 2-gram模型\n",
    "    for i, word in enumerate(words[:-1]):\n",
    "        # 获得当前词的下一个词\n",
    "        next_ = words[i+1]\n",
    "        probability = two_gram(word, next_)\n",
    "        sentence_pro *= probability\n",
    "    \n",
    "    return sentence_pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "generate_best 函数\n",
    "入参：\n",
    "    n:生成句子个数\n",
    "    gram_name：语法 \n",
    "        例子：eat_grammar\n",
    "    lm:语言模型\n",
    "        例子：'one_gram' / 'two_gram'\n",
    "能够生成n个句子，并能选择一个最合理的句子 \n",
    "'''\n",
    "def generate_best(n,gram_name,lm): \n",
    "#     调用generate_n函数生成句子\n",
    "    sentences = generate_n(n,gram_name)\n",
    "    probs = []\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if lm == 'one_gram':\n",
    "            probs.append([i,sentence,get_probablity_1gram(sentence)])\n",
    "        elif lm == 'two_gram':\n",
    "            probs.append([i,sentence,get_probablity_2gram(sentence)])\n",
    "        else:\n",
    "            print(\"请输入正确的语言模型配置\")\n",
    "    \n",
    "    print(\"*******生成随机的{0}个句子:*******\".format(n))\n",
    "    for num,sen,prob in probs:\n",
    "        print('第{0}个句子：\"{1}\"的概率为{2}'.format(num,sen,prob))\n",
    "    res = sorted(probs, key=lambda x: x[2],reverse=True)\n",
    "    \n",
    "    print(\"*******其中最合理的句子为*******\")\n",
    "    print('第{0}个句子：\"{1}\"，其概率为{2}'.format(res[0][0],res[0][1],res[0][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******生成随机的10个句子:*******\n",
      "第0个句子：\"傍晚我们去看动漫展嚒？\"的概率为1.2794425054372854e-33\n",
      "第1个句子：\"今天傍晚你去看展览或者去看动漫展嚒？\"的概率为2.4631691961279168e-51\n",
      "第2个句子：\"后天傍晚他们去吃烤肉或者去游乐场玩嚒？\"的概率为5.707360386179829e-56\n",
      "第3个句子：\"明天中午我们去看展览吧？\"的概率为4.709447432906284e-33\n",
      "第4个句子：\"今天晚上你去看动漫展然后再去游乐场玩吧？\"的概率为9.026185351358535e-50\n",
      "第5个句子：\"晚上你去看动漫展吗？\"的概率为9.631246404033203e-29\n",
      "第6个句子：\"明天中午他们去逛街然后再去吃烤肉嚒？\"的概率为7.716122847070974e-54\n",
      "第7个句子：\"明天晚上你去吃火锅吗？\"的概率为8.600237074673111e-30\n",
      "第8个句子：\"今天晚上我们去打球吧？\"的概率为1.4909285752540236e-29\n",
      "第9个句子：\"今天我们去看动漫展或者去看电影吗？\"的概率为1.3446529078440599e-39\n",
      "*******其中最合理的句子为*******\n",
      "第5个句子：\"晚上你去看动漫展吗？\"，其概率为9.631246404033203e-29\n"
     ]
    }
   ],
   "source": [
    "# 使用one_gram模型验证用eat_grammar语法生成的句子\n",
    "generate_best(10,eat_grammar,'one_gram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******生成随机的10个句子:*******\n",
      "第0个句子：\"晚上你去打球然后再去吃杭帮菜或者去吃小龙虾吧？\"的概率为1.3659327343994087e-63\n",
      "第1个句子：\"我们去吃杭帮菜然后再去看展览吗？\"的概率为4.74690559389097e-43\n",
      "第2个句子：\"中午他们去看动漫展吧？\"的概率为7.802219146872236e-31\n",
      "第3个句子：\"明天晚上我们去看展览嚒？\"的概率为3.0676632602153198e-28\n",
      "第4个句子：\"今天晚上我们去打球然后再去吃烤肉或者去看电影吗？\"的概率为1.948428636386017e-51\n",
      "第5个句子：\"后天晚上我们去逛街吗？\"的概率为2.643697090813043e-29\n",
      "第6个句子：\"后天傍晚你去吃烤肉然后再去游乐场玩吗？\"的概率为5.7354557928275935e-55\n",
      "第7个句子：\"傍晚他们去看动漫展然后再去看动漫展吗？\"的概率为2.746925429732777e-48\n",
      "第8个句子：\"明天晚上他们去爬山然后再去游乐场玩或者去逛街吗？\"的概率为7.011248895920466e-69\n",
      "第9个句子：\"明天我们去吃杭帮菜或者去逛街嚒？\"的概率为1.1121439148421458e-45\n",
      "*******其中最合理的句子为*******\n",
      "第3个句子：\"明天晚上我们去看展览嚒？\"，其概率为3.0676632602153198e-28\n"
     ]
    }
   ],
   "source": [
    "# 使用one_gram模型验证用eat_grammar语法生成的句子\n",
    "generate_best(10,eat_grammar,'two_gram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好了，现在我们实现了自己的第一个AI模型，这个模型能够生成比较接近于人类的语言。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: 这个模型有什么问题？ 你准备如何提升？ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在词库里的词有 ['今天', '明天', '后天', '中午', '晚上', '傍晚', '火锅', '骑车', '逛街', '爬山', '打球']\n",
      "不在词库里的词有 ['小龙虾', '游乐场玩']\n"
     ]
    }
   ],
   "source": [
    "original_words = ['今天','明天','后天','中午','晚上','傍晚','小龙虾','火锅','骑车','逛街','游乐场玩','爬山','打球']\n",
    "in_token = []\n",
    "not_in_token = []\n",
    "for w in original_words:\n",
    "    if w in TOKEN_1_GRAM:\n",
    "        in_token.append(w)\n",
    "    else:\n",
    "        not_in_token.append(w)\n",
    "print('在词库里的词有',in_token)\n",
    "print('不在词库里的词有',not_in_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "+ 1.如上所示，语法树里的一些词不在词库里，即使语法语意正确，在计算概率时候只会设置为1/len(token)，导致降低该生成句子的概率影响结果。\n",
    "    + 解决方案：需要扩展词库\n",
    "+ 2.one_gram和two_gram可能不能满足词上下文关联性，可以设置triple_gram或者更多的前后次关联起来进行计算\n",
    "+ 3.对于one-gram的词，有一些词频数高，但是区分度不高，比如“的”，如果造出来的句子是“的的的”会得到很高概率但是没有意义\n",
    "    + 解决方案：对停用词进行处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

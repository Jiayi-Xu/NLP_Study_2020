{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.复习上课内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.回答以下理论问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 简述skip-gram和cbow的区别 ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ cbow是用周围词预测中心词，从而利用中心词的预测结果情况\n",
    "+ skip-gram是用中心词来预测周围的词\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 缺少预料的情况下skip-gram和cbow哪个模型训练效果会更好？ 为什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当数据量较少时候，优先选择skip-gram，相比cbow会有更多的样本可以训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 negative sampling怎么减少训练的时间复杂度？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本来有V个词来预测一个词，负采样从其它V-1个词挑选K个词当作负样本，变成利用这一个正例和k个负例来进行训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4 Glove的基本想法是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "glove考虑全局语义的相关性，通过概率之间的比值来反应词之间的相关性，相关的话比值会大一些。基于全局语料库、并结合上下文语境构建词向量，结合了LSA和word2vec的优点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5 本节课所学的skip-gram, cbow 和glove 词向量有什么缺点 ？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这些方法得到的词向量是固定表征的，无法解决一词多义问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.实践题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本题，需要你利用gensim库训练文本的词向量并对一些词进行可视化展示所学词向量能反应语意信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据下载地址：https://dumps.wikimedia.org/zhwiki/20200301/zhwiki-20200301-pages-articles-multistream.xml.bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你需要用维基百科提供的extractor来提取数据：https://github.com/attardi/wikiextractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可能需要参考gensim的文档：https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可能需要使用t-sne做可视化：https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提取wiki数据\n",
    "python WikiExtractor.py -o extracted zhwiki-20200301-pages-articles-multistream.xml.bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取出的语料中繁简混杂，使用opencc将繁体部分也转换为简体\n",
    "import os\n",
    "data_path = \"./datasets/wikiextractor/extracted\"\n",
    "commands = []\n",
    "filename_jian_list = []\n",
    "for root, dirs, files in os.walk(data_path):\n",
    "    for filename in files:\n",
    "        input_filename = root + '/' + filename\n",
    "        output_filename = root + '/' + filename + '_jian'\n",
    "        commands.append(\"opencc -i {0} -o {1} -c t2s.json\".format(input_filename,output_filename))\n",
    "        filename_jian_list.append(output_filename)\n",
    "# 写入命令到comds.sh在终端执行\n",
    "f = open('comds.sh', 'w')\n",
    "for line in commands:\n",
    "    f.write(line)\n",
    "    f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# 定义去掉空格换行符的函数\n",
    "def token(raw_html):\n",
    "#     使用正则去掉每个文件开头的标签\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', raw_html)\n",
    "    return re.findall(r'[\\d|\\w]+',cleantext)\n",
    "\n",
    "# 定义读取停用词文件的函数\n",
    "def read_stopwords(path):\n",
    "    lines = set()\n",
    "    with open(path, mode='r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            lines.add(line)\n",
    "    return lines\n",
    "# 移除停用词\n",
    "STOP_WORDS = read_stopwords(\"./datasets/stop_words.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用jieba进行分词 并存储句子到sentences\n",
    "import jieba\n",
    "sentences = []\n",
    "for file_path in filename_jian_list:\n",
    "    for line in open(file_path):\n",
    "        raw_data = line.strip()\n",
    "        # 去掉开头的<>和空格换行符\n",
    "        data = token(raw_data)\n",
    "        data = ' '.join(data)\n",
    "        if data == '':\n",
    "            continue\n",
    "        # jieba.lcut 直接生成的就是一个list\n",
    "        word_seq = jieba.lcut(data.strip())\n",
    "        # 移除停用词\n",
    "        seg_list = [word for word in word_seq if word.strip() not in STOP_WORDS]\n",
    "        sentences.append(seg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentences = [' '.join(n) for n in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./datasets/sentences_data.text', 'w')\n",
    "for line in new_sentences:\n",
    "    f.write(line)\n",
    "    f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['腐殖质',\n",
       " '腐殖质 土壤 特异 有机质 土壤 有机质 组成部分 约 占 有机质 总量 50 65',\n",
       " '腐殖质 一种 分子 抗 分解 性强 棕色 或暗 棕色 无定形 胶体 动植物 残体 植物 组织 枯枝 落叶 动物 排泄物 皮毛 尸体 微生物 分解 转化 合成 一类 有机 高分子 化合物',\n",
       " '胡敏酸 富里 酸 含量 比例 土壤 而异 整体 黑色 褐色 无 定型 适度 粘结性 使 粘土 疏松 粘土 粘结 团粒结构 胶结剂',\n",
       " '养分 生物 循环 中 生物 死亡 后 生物 残 体会 矿物 化 过程 转化成 矿质 养分 生物 残体 会 矿物 化 生物 残 体会 腐殖 化 过程 腐殖质 形式 保留 养分 生物 循环 中 生物 残体 转化成 矿质 养分 一个 养分 储藏室 腐殖质 最终 会 矿物 化 过程 转化成 矿质 养分 腐殖质 雨水 冲走 泥土 上 矿质 养分 时 微生物 未能 分解 生物 残体 补充 微生物 分解 生物 残体 时间 很 慢 腐殖质 土壤 养分 浓度 释出 养分 补充 肥力 作用',\n",
       " '土壤 腐殖质 碳 氢 氧 氮 硫 磷 营养元素 元素 含量 看 胡敏酸 含碳 氮 硫较 富里 酸 高 氧 含量 较富里 酸 低 腐殖质 中 含氮 组分 形态 蛋白质 N 肽 N 氨基酸 N NH3 N 嘌呤 嘧啶 杂环 结构 上 N 环境 条件 下 腐殖质 含氮 差异 热带 土壤 中有 酸性 氨基酸 北极 土壤 中 这类 氨基酸 含量 低 热带 土壤 含 碱性 氨基酸 土壤 少',\n",
       " '实验 观察 土壤 中 氨基酸 细菌 氨基酸 土壤 蛋白质 肽 蛋白质 起源于 微生物',\n",
       " '腐殖质 非常复杂 生化 过程 3 种 学说',\n",
       " '土壤 保存 生物 残体 腐殖 化 养分 外 胶体 物质 特色 去 吸收 植物 缺少 阳离子 如钾 钙 镁',\n",
       " '阳离子 中铁 离子 腐殖质 土壤 颜色 染色 物质 土壤 颜色 去 分析 土壤 阳离子 成分']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看分词后的结果\n",
    "new_sentences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练文本的词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import multiprocessing\n",
    "import sys\n",
    "# 采用skip-gram算法\n",
    "sentence_path = './datasets/sentences_data.text'\n",
    "model = Word2Vec(sg=1, sentences=LineSentence(sentence_path),\n",
    "                               size=200,\n",
    "                               window=5,\n",
    "                               min_count=10,\n",
    "                               workers=multiprocessing.cpu_count())\n",
    "# 以该方式保存的模型可以在读取后进行再训练（追加训练），因为保存了训练的全部信息\n",
    "model.save(\"./datasets/model/word2vec_gensim\")\n",
    "# 保存词向量文件\n",
    "model.wv.save_word2vec_format(\"./datasets/model/word2vec_org\",\n",
    "                              \"./datasets/model/vocabulary\",\n",
    "                              binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./datasets/model/word2vec_gensim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format(\"./datasets/model/word2vec_org\",\n",
    "                              \"./datasets/model/vocabulary\",\n",
    "                              binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 434361 samples in 29.860s...\n"
     ]
    }
   ],
   "source": [
    "# 可视化\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    " \n",
    "#因为词向量文件比较大 随机抽取一些词可视化\n",
    "words = list(model.wv.vocab)\n",
    "random.shuffle(words)\n",
    "\n",
    "vector = model[words]\n",
    "tsne = TSNE(n_components=2,init='pca',verbose=1)\n",
    "embedd = tsne.fit_transform(vector)\n",
    " \n",
    "#可视化\n",
    "plt.figure(figsize=(14,10))\n",
    "plt.scatter(embedd[:300,0], embedd[:300,1])\n",
    " \n",
    "for i in range(300):\n",
    "    x = embedd[i][0]\n",
    "    y = embedd[i][1]\n",
    "    plt.text(x, y, words[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment-05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同学们，今天我们的学习了基本的机器学习概念，相比你已经对机器学习的这些方法有一个基本的认识了。值得说明的是，机器学习不仅仅是一系列方法，更重要的是一种思维体系，即：依据以往的、现有的数据，构建某种方法来解决未见过的问题。而且决策树，贝叶斯只是实现这个目标的一个方法，包括之后的神经网络。很有可能有一天，神经网络也会被淘汰，但是重要的是我们要理解机器学习的目标，就是尽可能的自动化解决未知的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-1 Programming Review 编程回顾"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Re-code the Linear-Regression Model using scikit-learning(10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you code here\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X.reshape(-1, 1), y)\n",
    "\n",
    "def lin_func(x): \n",
    "    return linreg.coef_ * x + linreg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Complete the unfinished KNN Model using pure python to solve the previous Line-Regression problem. (8 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you code here\n",
    "def predict(x, k=5):\n",
    "    most_similars = sorted(model(X, y), key=lambda xi: distance(xi[0], x))[:k]\n",
    "    #code here\n",
    "    # 存储最近k个的label\n",
    "    label_lst = [tup[1] for tup in most_similars]\n",
    "    # 投票统计次数最多的\n",
    "    label = Counter(label_lst).most_common()[0][0]\n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Re-code the Decision Tree, which could sort the features by salience. (12 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you code here\n",
    "\n",
    "# 修改element->counter\n",
    "def entropy(elements):\n",
    "    counter = Counter(elements)\n",
    "    probs = [counter[c] / len(elements) for c in counter]\n",
    "    return - sum(p * np.log2(p) for p in probs)\n",
    "\n",
    "# 按比例加和\n",
    "total = len(sub_spliter_1 + sub_spliter_2)\n",
    "entropy_v = len(sub_spliter_1)/total*entropy_1 + len(sub_spliter_2)/total*entropy_2  # change here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-2 Question and Answer 问答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What's the *model*? why  all the models are wrong, but some are useful? (5 points) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "+ model: 是对数据的分布规则进行假设去拟合\n",
    "\n",
    "+ 每个model都建立在一定的假设之上，所以所有的model均不能适用于所有的情况之下。只有在假设被满足时，也就是特定的情况下，可以对该特定情况的前因后果及其路径进行大致有用的描述。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What's the underfitting and overfitting? List the reasons that could make model overfitting or underfitting. (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "+ 过拟合（Overfitting）：指一个假设在训练数据上能够获得比其他假设更好的拟合（训练误差小）但是在训练数据外的数据集上却不能很好的拟合数据（测试误差大）。过拟合学到了很多没必要的特征。此时模型的泛化能力较差，不利于推广。\n",
    "    + 产生原因有：\n",
    "        + 参数过多，为了降低loss（神经网络的任务就是为了最小化loss），后者样本过少。参数/样本的比太大\n",
    "        \n",
    "\n",
    "+ 过拟合（underfitting）：回归问题线性拟合较差，分类问题则分类较差。可能训练样本被提取的特征比较少，导致训练出来的模型不能很好地匹配，表现得很差，甚至样本本身都无法高效的识别。\n",
    "    + 产生原因有：\n",
    "        + 可能训练样本被提取的特征比较少"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What's the precision, recall, AUC, F1, F2score. What are they mainly target on? (12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float:center;\" src=\"./datasets/confusion.png\" width='70%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "$$ precision =  \\frac{TP}{TP+FP}$$\n",
    "\n",
    "precision 为预测为正样本的结果里预测正确的频率值\n",
    "\n",
    "$$ recall =  \\frac{TP}{TP+FN}$$\n",
    "\n",
    "recall 为真实为正样本的结果有多少比例被预测出来\n",
    "\n",
    "AUC：Area under Curve（曲线下的面积）是一个模型评价指标，用于二分类模型的评价。这条“Curve（曲线）”就是ROC曲线。\n",
    "机器学习中的很多模型对于分类问题的预测结果大多是概率，即属于某个类别的概率，如果计算准确率的话，就要把概率转化为类别，这就需要设定一个阈值，概率大于某个阈值的属于一类，概率小于某个阈值的属于另一类，而阈值的设定直接影响了准确率的计算。\n",
    "\n",
    "\n",
    "F-measure值是用来综合考虑precision和recall的值，\n",
    "\n",
    "$$ F-measure = （1+\\beta^2)* \\frac{Precision*Recall}{\\beta^2*(Precision + Recall)}$$\n",
    "\n",
    "\n",
    "当 $\\beta$ =1时，成为F1-Score，这时召回率和精确率都很重要，权重相同。当有些情况下我们认为精确率更为重要，那就调整 β 的值小于 1 ，如果我们认为召回率更加重要，那就调整 $\\beta$的值大于1，比如F2-Score。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Based on our course and yourself mind, what's the machine learning?  (8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "帮助人们做决策， 基于一定的数据量，有一定的规则模式存在于数据中，利用机器学习的各种方法去挖掘数据的关系。\n",
    "\n",
    "通过设计模型，计算机可以自动地从提供的数据中学习再进行决策。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. \"正确定义了机器学习模型的评价标准(evaluation)， 问题基本上就已经解决一半\". 这句话是否正确？你是怎么看待的？ (8‘)\n",
    "\n",
    "Ans:\n",
    "\n",
    "认同这句话， 因为正确的定义评价标准可以帮助衡量模型的训练结果，让程序往预测结果和实际结果更加接近的方向调整。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-03 Programming Practice 编程练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In our course and previous practice, we complete some importance components of Decision Tree. In this problem, you need to build a **completed** Decision Tree Model. You show finish a `predicate()` function, which accepts three parameters **<gender, income, family_number>**, and outputs the predicated 'bought': 1 or 0.  (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "mock_data = {\n",
    "    'gender':['F', 'F', 'F', 'F', 'M', 'M', 'M'],\n",
    "    'income': ['+10', '-10', '+10', '+10', '+10', '+10', '-10'],\n",
    "    'family_number': [1, 1, 2, 1, 1, 1, 2],\n",
    "    'bought': [1, 1, 1, 0, 0, 0, 1],\n",
    "}\n",
    "dataset = pd.DataFrame.from_dict(mock_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def entropy(elements):\n",
    "    counter = Counter(elements)\n",
    "#     probs = [counter[c] / len(elements) for c in elements]\n",
    "    probs = [counter[c] / len(elements) for c in counter]\n",
    "#     print(probs)\n",
    "    return - sum(p * np.log2(p) for p in probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_the_min_spilter(training_data: pd.DataFrame, target: str) -> str:\n",
    "    x_fields = set(training_data.columns.tolist()) - {target}\n",
    "    \n",
    "    spliter = None\n",
    "    min_entropy = float('inf')\n",
    "    \n",
    "    for f in x_fields:\n",
    "        values = set(training_data[f])\n",
    "        for v in values:\n",
    "            sub_spliter_1 = training_data[training_data[f] == v][target].tolist()\n",
    "            entropy_1 = entropy(sub_spliter_1)\n",
    "            sub_spliter_2 = training_data[training_data[f] != v][target].tolist()\n",
    "            entropy_2 = entropy(sub_spliter_2)\n",
    "            # 按比例加和\n",
    "            total = len(sub_spliter_1 + sub_spliter_2)\n",
    "            entropy_v = len(sub_spliter_1)/total*entropy_1 + len(sub_spliter_2)/total*entropy_2  # change here \n",
    "            \n",
    "            if entropy_v <= min_entropy:\n",
    "                min_entropy = entropy_v\n",
    "                spliter = (f, v)\n",
    "    \n",
    "    print('spliter is: {}'.format(spliter))\n",
    "    print('the min entropy is: {}'.format(min_entropy))\n",
    "    \n",
    "    return spliter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you code here\n",
    "# 定义majorityCnt 对最后的分类结果进行选择\n",
    "def majorityCnt(classList):    #按分类后类别数量排序\n",
    "    classCount={}\n",
    "    for vote in classList:\n",
    "        if vote not in classCount.keys():\n",
    "            classCount[vote]=0\n",
    "        classCount[vote]+=1\n",
    "    sortedClassCount = sorted(classCount.items(),key=lambda x:x[1],reverse=True)\n",
    "    return sortedClassCount[0][0]\n",
    "\n",
    "def predicate(dataset,labels):\n",
    "    classList= dataset['bought'].values.tolist()  # 最终类别\n",
    "    if classList.count(classList[0])==len(classList):\n",
    "        return classList[0]\n",
    "    if len(dataset.iloc[0])==1:\n",
    "        return majorityCnt(classList)\n",
    "    #选择最优特征\n",
    "    bestFeat=find_the_min_spilter(dataset,'bought')[0]\n",
    "    #分类结果以字典形式保存\n",
    "    myTree = {bestFeat:{}} \n",
    "    # 删除已经被选择的feature\n",
    "    labels.remove(bestFeat)\n",
    "    # 提取最优feature下的值 \n",
    "    featValues = dataset[bestFeat].values.tolist()\n",
    "    uniqueVals=set(featValues)\n",
    "    # 对最优feature的不同value进行拆分继续处理\n",
    "    for value in uniqueVals:\n",
    "        subDataset = dataset[dataset[bestFeat]== value].drop(bestFeat,axis=1)\n",
    "        subLabels = labels[:]\n",
    "        myTree[bestFeat][value] = predicate(subDataset,subLabels)\n",
    "    return myTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spliter is: ('income', '-10')\n",
      "the min entropy is: 0.6935361388961918\n",
      "spliter is: ('gender', 'M')\n",
      "the min entropy is: 0.5509775004326937\n",
      "spliter is: ('family_number', 2)\n",
      "the min entropy is: 0.6666666666666666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'income': {'+10': {'gender': {'F': {'family_number': {1: 1, 2: 1}}, 'M': 0}},\n",
       "  '-10': 1}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = dataset.columns.tolist()\n",
    "labels.remove('bought')\n",
    "# 将最终决策树分类结果打印出来\n",
    "predicate(dataset, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
